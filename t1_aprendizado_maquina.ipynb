{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDi1qUQHry2k"
      },
      "source": [
        "# Primeiro Prompt: k-NN\n",
        "Implemente um algoritmo do Knn em python sem usar scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XGkKK6ory2m"
      },
      "source": [
        "Aqui está uma implementação básica do algoritmo K-Nearest Neighbors (KNN) em Python, sem o uso da biblioteca `scikit-learn`. Esse código implementa o KNN de forma simples e eficiente, usando apenas bibliotecas padrão do Python como `math` e `collections` para calcular as distâncias e determinar os vizinhos mais próximos.\n",
        "\n",
        "### Passos principais do KNN:\n",
        "1. Calcular a distância entre o ponto de teste e todos os pontos de treino.\n",
        "2. Selecionar os `k` vizinhos mais próximos com base nas distâncias.\n",
        "3. Realizar a votação majoritária (para classificação) ou calcular a média (para regressão) entre os vizinhos selecionados.\n",
        "\n",
        "Aqui está o código de exemplo:\n",
        "\n",
        "```python\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# Função para calcular a distância Euclidiana entre dois pontos\n",
        "def euclidean_distance(point1, point2):\n",
        "    return math.sqrt(sum((x - y) ** 2 for x, y in zip(point1, point2)))\n",
        "\n",
        "# Implementação do algoritmo KNN\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "        self.data = None\n",
        "        self.labels = None\n",
        "\n",
        "    # Função para treinar o modelo com os dados de treino\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.data = X_train\n",
        "        self.labels = y_train\n",
        "\n",
        "    # Função para prever o rótulo (classificação)\n",
        "    def predict(self, X_test):\n",
        "        predictions = []\n",
        "        for test_point in X_test:\n",
        "            # Calcular a distância de todos os pontos de treino\n",
        "            distances = [(euclidean_distance(test_point, train_point), label)\n",
        "                         for train_point, label in zip(self.data, self.labels)]\n",
        "            # Ordenar as distâncias e pegar os k vizinhos mais próximos\n",
        "            sorted_distances = sorted(distances)[:self.k]\n",
        "            # Coletar os rótulos dos vizinhos mais próximos\n",
        "            k_nearest_labels = [label for _, label in sorted_distances]\n",
        "            # Votação majoritária\n",
        "            most_common_label = Counter(k_nearest_labels).most_common(1)[0][0]\n",
        "            predictions.append(most_common_label)\n",
        "        return predictions\n",
        "\n",
        "# Exemplo de uso\n",
        "if __name__ == \"__main__\":\n",
        "    # Dados de treino (4 amostras de 2 features)\n",
        "    X_train = [[1, 2], [2, 3], [3, 4], [5, 6]]\n",
        "    y_train = [0, 0, 1, 1]  # Rótulos correspondentes\n",
        "\n",
        "    # Dados de teste (1 amostra de 2 features)\n",
        "    X_test = [[1.5, 2.5]]\n",
        "\n",
        "    # Instancia o modelo KNN\n",
        "    knn = KNN(k=3)\n",
        "    \n",
        "    # Treina o modelo\n",
        "    knn.fit(X_train, y_train)\n",
        "    \n",
        "    # Faz a previsão\n",
        "    prediction = knn.predict(X_test)\n",
        "    \n",
        "    print(\"Predição:\", prediction)\n",
        "```\n",
        "\n",
        "### Explicação:\n",
        "- **euclidean_distance**: Calcula a distância euclidiana entre dois pontos.\n",
        "- **KNN**:\n",
        "  - `__init__(self, k=3)`: Inicializa o modelo com o número de vizinhos `k`.\n",
        "  - `fit(self, X_train, y_train)`: Armazena os dados de treino e seus rótulos.\n",
        "  - `predict(self, X_test)`: Faz a previsão para novos dados de teste, calculando a distância de cada ponto de teste para todos os pontos de treino, selecionando os `k` mais próximos e retornando o rótulo mais comum (votação majoritária).\n",
        "\n",
        "Você pode alterar a função de distância para outras métricas, como distância de Manhattan, ou ajustar o valor de `k` conforme necessário.\n",
        "\n",
        "Se precisar de mais informações ou ajustes, estou à disposição!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0sGyHN3ry2n"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# Função para calcular a distância Euclidiana entre dois pontos\n",
        "def euclidean_distance(point1, point2):\n",
        "    return math.sqrt(sum((x - y) ** 2 for x, y in zip(point1, point2)))\n",
        "\n",
        "# Implementação do algoritmo KNN\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "        self.data = None\n",
        "        self.labels = None\n",
        "\n",
        "    # Função para treinar o modelo com os dados de treino\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.data = X_train\n",
        "        self.labels = y_train\n",
        "\n",
        "    # Função para prever o rótulo (classificação)\n",
        "    def predict(self, X_test):\n",
        "        predictions = []\n",
        "        for test_point in X_test:\n",
        "            # Calcular a distância de todos os pontos de treino\n",
        "            distances = [(euclidean_distance(test_point, train_point), label)\n",
        "                         for train_point, label in zip(self.data, self.labels)]\n",
        "            # Ordenar as distâncias e pegar os k vizinhos mais próximos\n",
        "            sorted_distances = sorted(distances)[:self.k]\n",
        "            # Coletar os rótulos dos vizinhos mais próximos\n",
        "            k_nearest_labels = [label for _, label in sorted_distances]\n",
        "            # Votação majoritária\n",
        "            most_common_label = Counter(k_nearest_labels).most_common(1)[0][0]\n",
        "            predictions.append(most_common_label)\n",
        "        return predictions\n",
        "\n",
        "# Exemplo de uso\n",
        "if __name__ == \"__main__\":\n",
        "    # Dados de treino (4 amostras de 2 features)\n",
        "    X_train = [[1, 2], [2, 3], [3, 4], [5, 6]]\n",
        "    y_train = [0, 0, 1, 1]  # Rótulos correspondentes\n",
        "\n",
        "    # Dados de teste (1 amostra de 2 features)\n",
        "    X_test = [[1.5, 2.5]]\n",
        "\n",
        "    # Instancia o modelo KNN\n",
        "    knn = KNN(k=3)\n",
        "\n",
        "    # Treina o modelo\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Faz a previsão\n",
        "    prediction = knn.predict(X_test)\n",
        "\n",
        "    print(\"Predição:\", prediction)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLloGEmEry2p"
      },
      "source": [
        "# Segundo prompt: Naive Bayes\n",
        "Implemente um algoritmo de Naive Bayes em python sem usar scikit-learn, ou seja, from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L833wrL1ry2p"
      },
      "source": [
        "Aqui está uma implementação básica de um classificador Naive Bayes \"from scratch\" em Python, sem usar `scikit-learn`. Essa abordagem é baseada no teorema de Bayes e na suposição de que as características são independentes entre si (daí o nome \"Naive\").\n",
        "\n",
        "### Passos principais:\n",
        "1. **Treinamento**: Calcular a probabilidade condicional de cada classe com base nas características (estimativas de máxima verossimilhança).\n",
        "2. **Predição**: Para cada novo exemplo, calcular a probabilidade posterior de cada classe, dada a entrada, e selecionar a classe com a maior probabilidade.\n",
        "\n",
        "Para simplificar, vamos implementar a versão para classificação binária ou multiclasse, assumindo que os atributos são contínuos (usaremos a função densidade de probabilidade Gaussiana).\n",
        "\n",
        "### Implementação:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "class NaiveBayes:\n",
        "    def fit(self, X_train, y_train):\n",
        "        # Separar por classe\n",
        "        self.classes = np.unique(y_train)\n",
        "        self.mean = {}\n",
        "        self.var = {}\n",
        "        self.priors = {}\n",
        "        \n",
        "        # Para cada classe\n",
        "        for c in self.classes:\n",
        "            X_c = X_train[y_train == c]\n",
        "            self.mean[c] = np.mean(X_c, axis=0)\n",
        "            self.var[c] = np.var(X_c, axis=0)\n",
        "            self.priors[c] = X_c.shape[0] / X_train.shape[0]\n",
        "\n",
        "    # Função para calcular a densidade de probabilidade Gaussiana\n",
        "    def gaussian_pdf(self, class_idx, x):\n",
        "        mean = self.mean[class_idx]\n",
        "        var = self.var[class_idx]\n",
        "        numerator = np.exp(-(x - mean) ** 2 / (2 * var))\n",
        "        denominator = np.sqrt(2 * np.pi * var)\n",
        "        return numerator / denominator\n",
        "    \n",
        "    # Função para calcular a probabilidade posterior\n",
        "    def predict(self, X_test):\n",
        "        y_pred = [self._predict(x) for x in X_test]\n",
        "        return np.array(y_pred)\n",
        "    \n",
        "    # Função interna que calcula a probabilidade posterior para um exemplo\n",
        "    def _predict(self, x):\n",
        "        posteriors = []\n",
        "        \n",
        "        for c in self.classes:\n",
        "            prior = np.log(self.priors[c])  # Probabilidade a priori\n",
        "            conditional = np.sum(np.log(self.gaussian_pdf(c, x)))  # Verossimilhança\n",
        "            posterior = prior + conditional  # Posterior\n",
        "            posteriors.append(posterior)\n",
        "        \n",
        "        return self.classes[np.argmax(posteriors)]  # Retorna a classe com maior probabilidade\n",
        "\n",
        "# Exemplo de uso\n",
        "if __name__ == \"__main__\":\n",
        "    # Exemplo de dados: 2 features\n",
        "    X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
        "    y_train = np.array([0, 0, 1, 1, 1])\n",
        "\n",
        "    X_test = np.array([[1.5, 2.5], [3.5, 4.5]])\n",
        "\n",
        "    # Instancia o classificador Naive Bayes\n",
        "    nb = NaiveBayes()\n",
        "    \n",
        "    # Treina o modelo\n",
        "    nb.fit(X_train, y_train)\n",
        "    \n",
        "    # Faz a predição\n",
        "    predictions = nb.predict(X_test)\n",
        "    \n",
        "    print(\"Predições:\", predictions)\n",
        "```\n",
        "\n",
        "### Explicação:\n",
        "1. **`fit(self, X_train, y_train)`**:\n",
        "   - Calcula a média, a variância e as probabilidades a priori para cada classe. A suposição é que as características seguem uma distribuição normal (Gaussiana), então armazenamos a média e a variância de cada característica para cada classe.\n",
        "   \n",
        "2. **`gaussian_pdf(self, class_idx, x)`**:\n",
        "   - Função que calcula a densidade de probabilidade Gaussiana, dada uma amostra, para uma determinada classe. Isso é utilizado para calcular a verossimilhança de uma amostra pertencer a uma classe.\n",
        "\n",
        "3. **`predict(self, X_test)`**:\n",
        "   - Calcula a probabilidade posterior para cada classe e retorna a classe com a maior probabilidade.\n",
        "\n",
        "4. **_Exemplo de uso_**:\n",
        "   - No final, temos um exemplo de como o algoritmo pode ser utilizado, com um conjunto de treino simples e um conjunto de teste para fazer a predição.\n",
        "\n",
        "### Fórmulas:\n",
        "- A densidade de probabilidade Gaussiana para cada atributo \\( x_i \\) é calculada como:\n",
        "  \n",
        "  \\[\n",
        "  P(x_i|c) = \\frac{1}{\\sqrt{2\\pi \\sigma_c^2}} \\exp\\left(-\\frac{(x_i - \\mu_c)^2}{2\\sigma_c^2}\\right)\n",
        "  \\]\n",
        "  \n",
        "  Onde \\( \\mu_c \\) e \\( \\sigma_c \\) são a média e a variância da classe \\( c \\) para a característica \\( x_i \\).\n",
        "\n",
        "- A probabilidade posterior é:\n",
        "  \n",
        "  \\[\n",
        "  P(c|x) \\propto P(c) \\prod_{i=1}^{n} P(x_i|c)\n",
        "  \\]\n",
        "\n",
        "Se precisar de ajustes ou mais explicações, me avise!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIyNDQyCry2q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class NaiveBayes:\n",
        "    def fit(self, X_train, y_train):\n",
        "        # Separar por classe\n",
        "        self.classes = np.unique(y_train)\n",
        "        self.mean = {}\n",
        "        self.var = {}\n",
        "        self.priors = {}\n",
        "\n",
        "        # Para cada classe\n",
        "        for c in self.classes:\n",
        "            X_c = X_train[y_train == c]\n",
        "            self.mean[c] = np.mean(X_c, axis=0)\n",
        "            self.var[c] = np.var(X_c, axis=0)\n",
        "            self.priors[c] = X_c.shape[0] / X_train.shape[0]\n",
        "\n",
        "    # Função para calcular a densidade de probabilidade Gaussiana\n",
        "    def gaussian_pdf(self, class_idx, x):\n",
        "        mean = self.mean[class_idx]\n",
        "        var = self.var[class_idx]\n",
        "        numerator = np.exp(-(x - mean) ** 2 / (2 * var))\n",
        "        denominator = np.sqrt(2 * np.pi * var)\n",
        "        return numerator / denominator\n",
        "\n",
        "    # Função para calcular a probabilidade posterior\n",
        "    def predict(self, X_test):\n",
        "        y_pred = [self._predict(x) for x in X_test]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    # Função interna que calcula a probabilidade posterior para um exemplo\n",
        "    def _predict(self, x):\n",
        "        posteriors = []\n",
        "\n",
        "        for c in self.classes:\n",
        "            prior = np.log(self.priors[c])  # Probabilidade a priori\n",
        "            conditional = np.sum(np.log(self.gaussian_pdf(c, x)))  # Verossimilhança\n",
        "            posterior = prior + conditional  # Posterior\n",
        "            posteriors.append(posterior)\n",
        "\n",
        "        return self.classes[np.argmax(posteriors)]  # Retorna a classe com maior probabilidade\n",
        "\n",
        "# Exemplo de uso\n",
        "if __name__ == \"__main__\":\n",
        "    # Exemplo de dados: 2 features\n",
        "    X_train = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
        "    y_train = np.array([0, 0, 1, 1, 1])\n",
        "\n",
        "    X_test = np.array([[1.5, 2.5], [3.5, 4.5]])\n",
        "\n",
        "    # Instancia o classificador Naive Bayes\n",
        "    nb = NaiveBayes()\n",
        "\n",
        "    # Treina o modelo\n",
        "    nb.fit(X_train, y_train)\n",
        "\n",
        "    # Faz a predição\n",
        "    predictions = nb.predict(X_test)\n",
        "\n",
        "    print(\"Predições:\", predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9LnZQtzry2r"
      },
      "source": [
        "# Terceiro prompt: Arvore de decisao\n",
        "Implemente um algoritmo de arvore de decisao em python sem usar scikit-leanr, ou seja, from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-WbYrHRry2r"
      },
      "source": [
        "Aqui está uma implementação de uma árvore de decisão do zero em Python, sem o uso da biblioteca `scikit-learn`. A implementação utiliza o critério de **Gini** para dividir os dados:\n",
        "\n",
        "### Passo 1: Cálculo do Índice Gini\n",
        "O índice Gini mede a impureza de uma divisão, ou seja, quão mista estão as classes.\n",
        "\n",
        "### Passo 2: Funções de Divisão\n",
        "Precisamos de uma função para dividir os dados com base em um valor em uma coluna específica.\n",
        "\n",
        "### Passo 3: Construção da Árvore\n",
        "A árvore de decisão é construída de forma recursiva, dividindo os dados até encontrar folhas puras ou atingir um critério de parada (como profundidade máxima).\n",
        "\n",
        "### Passo 4: Função de Previsão\n",
        "Depois que a árvore é construída, a função de previsão é utilizada para classificar novas amostras.\n",
        "\n",
        "### Código:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Função para calcular o índice Gini de um grupo de amostras\n",
        "def gini(groups, classes):\n",
        "    # Número total de amostras\n",
        "    n_instances = float(sum([len(group) for group in groups]))\n",
        "    \n",
        "    gini_index = 0.0\n",
        "    for group in groups:\n",
        "        size = len(group)\n",
        "        if size == 0:\n",
        "            continue\n",
        "        score = 0.0\n",
        "        # Proporção de cada classe no grupo\n",
        "        for class_val in classes:\n",
        "            p = [row[-1] for row in group].count(class_val) / size\n",
        "            score += p * p\n",
        "        gini_index += (1.0 - score) * (size / n_instances)\n",
        "    \n",
        "    return gini_index\n",
        "\n",
        "# Função para dividir os dados em dois grupos com base em um valor específico de uma coluna\n",
        "def test_split(index, value, dataset):\n",
        "    left, right = [], []\n",
        "    for row in dataset:\n",
        "        if row[index] < value:\n",
        "            left.append(row)\n",
        "        else:\n",
        "            right.append(row)\n",
        "    return left, right\n",
        "\n",
        "# Selecionar a melhor divisão do dataset\n",
        "def get_split(dataset):\n",
        "    class_values = list(set(row[-1] for row in dataset))\n",
        "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "    for index in range(len(dataset[0])-1):\n",
        "        for row in dataset:\n",
        "            groups = test_split(index, row[index], dataset)\n",
        "            gini_index = gini(groups, class_values)\n",
        "            if gini_index < b_score:\n",
        "                b_index, b_value, b_score, b_groups = index, row[index], gini_index, groups\n",
        "    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
        "\n",
        "# Criar um nó folha\n",
        "def to_terminal(group):\n",
        "    outcomes = [row[-1] for row in group]\n",
        "    return max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "# Dividir os nós, criando sub-árvores de forma recursiva ou folhas\n",
        "def split(node, max_depth, min_size, depth):\n",
        "    left, right = node['groups']\n",
        "    del(node['groups'])\n",
        "    # Verificar se não há divisão\n",
        "    if not left or not right:\n",
        "        node['left'] = node['right'] = to_terminal(left + right)\n",
        "        return\n",
        "    # Verificar profundidade máxima\n",
        "    if depth >= max_depth:\n",
        "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "        return\n",
        "    # Processar o nó esquerdo\n",
        "    if len(left) <= min_size:\n",
        "        node['left'] = to_terminal(left)\n",
        "    else:\n",
        "        node['left'] = get_split(left)\n",
        "        split(node['left'], max_depth, min_size, depth+1)\n",
        "    # Processar o nó direito\n",
        "    if len(right) <= min_size:\n",
        "        node['right'] = to_terminal(right)\n",
        "    else:\n",
        "        node['right'] = get_split(right)\n",
        "        split(node['right'], max_depth, min_size, depth+1)\n",
        "\n",
        "# Construir uma árvore de decisão\n",
        "def build_tree(train, max_depth, min_size):\n",
        "    root = get_split(train)\n",
        "    split(root, max_depth, min_size, 1)\n",
        "    return root\n",
        "\n",
        "# Fazer uma previsão com a árvore de decisão\n",
        "def predict(node, row):\n",
        "    if row[node['index']] < node['value']:\n",
        "        if isinstance(node['left'], dict):\n",
        "            return predict(node['left'], row)\n",
        "        else:\n",
        "            return node['left']\n",
        "    else:\n",
        "        if isinstance(node['right'], dict):\n",
        "            return predict(node['right'], row)\n",
        "        else:\n",
        "            return node['right']\n",
        "\n",
        "# Exemplo de uso\n",
        "if __name__ == '__main__':\n",
        "    # Dataset de exemplo: [característica1, característica2, ..., classe]\n",
        "    dataset = [[2.771244718, 1.784783929, 0],\n",
        "               [1.728571309, 1.169761413, 0],\n",
        "               [3.678319846, 2.81281357, 0],\n",
        "               [3.961043357, 2.61995032, 0],\n",
        "               [2.999208922, 2.209014212, 0],\n",
        "               [7.497545867, 3.162953546, 1],\n",
        "               [9.00220326, 3.339047188, 1],\n",
        "               [7.444542326, 0.476683375, 1],\n",
        "               [10.12493903, 3.234550982, 1],\n",
        "               [6.642287351, 3.319983761, 1]]\n",
        "\n",
        "    # Definindo os parâmetros da árvore\n",
        "    max_depth = 3\n",
        "    min_size = 1\n",
        "\n",
        "    # Construir a árvore\n",
        "    tree = build_tree(dataset, max_depth, min_size)\n",
        "\n",
        "    # Fazer previsões\n",
        "    for row in dataset:\n",
        "        prediction = predict(tree, row)\n",
        "        print('Esperado=%d, Previsto=%d' % (row[-1], prediction))\n",
        "```\n",
        "\n",
        "### Explicação:\n",
        "1. **gini**: Calcula o índice Gini para um conjunto de grupos.\n",
        "2. **test_split**: Divide o conjunto de dados em dois grupos com base no valor de uma característica.\n",
        "3. **get_split**: Encontra a melhor divisão possível do dataset, retornando o índice da coluna e o valor que proporcionam a menor impureza Gini.\n",
        "4. **split**: Realiza a divisão recursiva dos nós.\n",
        "5. **build_tree**: Constrói a árvore de decisão.\n",
        "6. **predict**: Faz previsões com base na árvore gerada.\n",
        "\n",
        "### Testando\n",
        "O código usa um pequeno dataset para testar a construção da árvore e as previsões feitas com ela."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CufyKdLbry2s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Função para calcular o índice Gini de um grupo de amostras\n",
        "def gini(groups, classes):\n",
        "    # Número total de amostras\n",
        "    n_instances = float(sum([len(group) for group in groups]))\n",
        "\n",
        "    gini_index = 0.0\n",
        "    for group in groups:\n",
        "        size = len(group)\n",
        "        if size == 0:\n",
        "            continue\n",
        "        score = 0.0\n",
        "        # Proporção de cada classe no grupo\n",
        "        for class_val in classes:\n",
        "            p = [row[-1] for row in group].count(class_val) / size\n",
        "            score += p * p\n",
        "        gini_index += (1.0 - score) * (size / n_instances)\n",
        "\n",
        "    return gini_index\n",
        "\n",
        "# Função para dividir os dados em dois grupos com base em um valor específico de uma coluna\n",
        "def test_split(index, value, dataset):\n",
        "    left, right = [], []\n",
        "    for row in dataset:\n",
        "        if row[index] < value:\n",
        "            left.append(row)\n",
        "        else:\n",
        "            right.append(row)\n",
        "    return left, right\n",
        "\n",
        "# Selecionar a melhor divisão do dataset\n",
        "def get_split(dataset):\n",
        "    class_values = list(set(row[-1] for row in dataset))\n",
        "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "    for index in range(len(dataset[0])-1):\n",
        "        for row in dataset:\n",
        "            groups = test_split(index, row[index], dataset)\n",
        "            gini_index = gini(groups, class_values)\n",
        "            if gini_index < b_score:\n",
        "                b_index, b_value, b_score, b_groups = index, row[index], gini_index, groups\n",
        "    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
        "\n",
        "# Criar um nó folha\n",
        "def to_terminal(group):\n",
        "    outcomes = [row[-1] for row in group]\n",
        "    return max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "# Dividir os nós, criando sub-árvores de forma recursiva ou folhas\n",
        "def split(node, max_depth, min_size, depth):\n",
        "    left, right = node['groups']\n",
        "    del(node['groups'])\n",
        "    # Verificar se não há divisão\n",
        "    if not left or not right:\n",
        "        node['left'] = node['right'] = to_terminal(left + right)\n",
        "        return\n",
        "    # Verificar profundidade máxima\n",
        "    if depth >= max_depth:\n",
        "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "        return\n",
        "    # Processar o nó esquerdo\n",
        "    if len(left) <= min_size:\n",
        "        node['left'] = to_terminal(left)\n",
        "    else:\n",
        "        node['left'] = get_split(left)\n",
        "        split(node['left'], max_depth, min_size, depth+1)\n",
        "    # Processar o nó direito\n",
        "    if len(right) <= min_size:\n",
        "        node['right'] = to_terminal(right)\n",
        "    else:\n",
        "        node['right'] = get_split(right)\n",
        "        split(node['right'], max_depth, min_size, depth+1)\n",
        "\n",
        "# Construir uma árvore de decisão\n",
        "def build_tree(train, max_depth, min_size):\n",
        "    root = get_split(train)\n",
        "    split(root, max_depth, min_size, 1)\n",
        "    return root\n",
        "\n",
        "# Fazer uma previsão com a árvore de decisão\n",
        "def predict(node, row):\n",
        "    if row[node['index']] < node['value']:\n",
        "        if isinstance(node['left'], dict):\n",
        "            return predict(node['left'], row)\n",
        "        else:\n",
        "            return node['left']\n",
        "    else:\n",
        "        if isinstance(node['right'], dict):\n",
        "            return predict(node['right'], row)\n",
        "        else:\n",
        "            return node['right']\n",
        "\n",
        "# Exemplo de uso\n",
        "if __name__ == '__main__':\n",
        "    # Dataset de exemplo: [característica1, característica2, ..., classe]\n",
        "    dataset = [[2.771244718, 1.784783929, 0],\n",
        "               [1.728571309, 1.169761413, 0],\n",
        "               [3.678319846, 2.81281357, 0],\n",
        "               [3.961043357, 2.61995032, 0],\n",
        "               [2.999208922, 2.209014212, 0],\n",
        "               [7.497545867, 3.162953546, 1],\n",
        "               [9.00220326, 3.339047188, 1],\n",
        "               [7.444542326, 0.476683375, 1],\n",
        "               [10.12493903, 3.234550982, 1],\n",
        "               [6.642287351, 3.319983761, 1]]\n",
        "\n",
        "    # Definindo os parâmetros da árvore\n",
        "    max_depth = 3\n",
        "    min_size = 1\n",
        "\n",
        "    # Construir a árvore\n",
        "    tree = build_tree(dataset, max_depth, min_size)\n",
        "\n",
        "    # Fazer previsões\n",
        "    for row in dataset:\n",
        "        prediction = predict(tree, row)\n",
        "        print('Esperado=%d, Previsto=%d' % (row[-1], prediction))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Aplicando as métricas de avaliação nos algoritmos obtidos"
      ],
      "metadata": {
        "id": "O0Fpih1GyJmH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Através dos algoritmos obtidos na primeira etapa, foi pedido para o chatGPT aplicar a leitura do *dataset* escolhido e acrescentar os métodos que gerassem a matriz de confusão e as métricas de avaliação acurácia, precisão, *recall* e f1-score.\n",
        "\n",
        "No chatGPT, junto a cada código, foi acrescido o seguinte pedido:\n",
        "> Preciso que no código acima, seja adicionado:\n",
        "1. Carregar os dados do dataset presente em /content/sample_data/penguins.csv. A primeira e segunda coluna sao strings, a terceira e quarta são floats, a quinta e a sexta coluna são integer e o rótulo na coluna 7 é uma string.\n",
        "2. Preciso que, após a predictions, seja feita a matriz de confusão, acurá-\n",
        "cia, recall, precisão e f1-score\n",
        "Observação:\n",
        "1.Não pode ser utilizada nenhuma biblioteca do scikit-learn\n",
        "2. Algumas colunas do float ou integer podem ser valores vazio\n",
        "3. Desconsideradar os rótulos que são nulos\n",
        "\n",
        "Os resultados obtidos para cada algoritmo podem ser vistos e executados nas sessões 'Testes com k-NN', 'Testes com Naive Bayes' e 'Testes com Árvores de Decisão'."
      ],
      "metadata": {
        "id": "UB5GLU4-yVvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testes com k-NN"
      ],
      "metadata": {
        "id": "R4M9f_KdygZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# Função para calcular a distância Euclidiana entre dois pontos\n",
        "def euclidean_distance(point1, point2):\n",
        "    return math.sqrt(sum((x - y) ** 2 for x, y in zip(point1, point2)))\n",
        "\n",
        "# Função para calcular a média ignorando valores vazios\n",
        "def mean_without_empty(values):\n",
        "    valid_values = [float(v) for v in values if v != '']\n",
        "    return sum(valid_values) / len(valid_values) if valid_values else 0\n",
        "\n",
        "# Função para calcular a moda ignorando valores vazios\n",
        "def mode_without_empty(values):\n",
        "    valid_values = [int(v) for v in values if v != '']\n",
        "    return Counter(valid_values).most_common(1)[0][0] if valid_values else 0\n",
        "\n",
        "# Função para ler e preprocessar os dados do CSV, desconsiderando rótulos nulos\n",
        "def load_and_preprocess_data(filepath):\n",
        "    with open(filepath, 'r') as file:\n",
        "        data = [line.strip().split(',') for line in file.readlines()[1:]]  # Ignorar cabeçalho\n",
        "\n",
        "    # Filtrar linhas onde o rótulo (coluna 7) não está vazio\n",
        "    data = [row for row in data if row[6] != '']\n",
        "\n",
        "    # Preencher valores faltantes (float e int)\n",
        "    col3 = [row[2] for row in data]\n",
        "    col4 = [row[3] for row in data]\n",
        "    col5 = [row[4] for row in data]\n",
        "    col6 = [row[5] for row in data]\n",
        "\n",
        "    mean_col3 = mean_without_empty(col3)\n",
        "    mean_col4 = mean_without_empty(col4)\n",
        "    mode_col5 = mode_without_empty(col5)\n",
        "    mode_col6 = mode_without_empty(col6)\n",
        "\n",
        "    # Converter as colunas de string em números (poderia ser feito com um mapeamento para labels)\n",
        "    str_columns_1_2 = {value: idx for idx, value in enumerate(sorted(set([row[0] for row in data] + [row[1] for row in data])))}\n",
        "\n",
        "    X = [[str_columns_1_2[row[0]], str_columns_1_2[row[1]],\n",
        "          float(row[2]) if row[2] != '' else mean_col3,\n",
        "          float(row[3]) if row[3] != '' else mean_col4,\n",
        "          int(row[4]) if row[4] != '' else mode_col5,\n",
        "          int(row[5]) if row[5] != '' else mode_col6] for row in data]\n",
        "    y = [row[6] for row in data]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Implementação do algoritmo KNN\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "        self.data = None\n",
        "        self.labels = None\n",
        "\n",
        "    # Função para treinar o modelo com os dados de treino\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.data = X_train\n",
        "        self.labels = y_train\n",
        "\n",
        "    # Função para prever o rótulo (classificação)\n",
        "    def predict(self, X_test):\n",
        "        predictions = []\n",
        "        for test_point in X_test:\n",
        "            # Calcular a distância de todos os pontos de treino\n",
        "            distances = [(euclidean_distance(test_point, train_point), label)\n",
        "                         for train_point, label in zip(self.data, self.labels)]\n",
        "            # Ordenar as distâncias e pegar os k vizinhos mais próximos\n",
        "            sorted_distances = sorted(distances)[:self.k]\n",
        "            # Coletar os rótulos dos vizinhos mais próximos\n",
        "            k_nearest_labels = [label for _, label in sorted_distances]\n",
        "            # Votação majoritária\n",
        "            most_common_label = Counter(k_nearest_labels).most_common(1)[0][0]\n",
        "            predictions.append(most_common_label)\n",
        "        return predictions\n",
        "\n",
        "# Função para calcular a matriz de confusão\n",
        "def confusion_matrix(y_true, y_pred):\n",
        "    unique_labels = sorted(set(y_true))\n",
        "    matrix = {label: {label: 0 for label in unique_labels} for label in unique_labels}\n",
        "\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        matrix[true][pred] += 1\n",
        "\n",
        "    return matrix\n",
        "\n",
        "# Funções para calcular acurácia, precisão, recall e F1-score\n",
        "def accuracy(y_true, y_pred):\n",
        "    correct = sum(1 for true, pred in zip(y_true, y_pred) if true == pred)\n",
        "    return correct / len(y_true)\n",
        "\n",
        "def precision_recall_f1(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    precisions = {}\n",
        "    recalls = {}\n",
        "    f1_scores = {}\n",
        "\n",
        "    for label in cm:\n",
        "        tp = cm[label][label]\n",
        "        fp = sum(cm[other][label] for other in cm if other != label)\n",
        "        fn = sum(cm[label][other] for other in cm if other != label)\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        precisions[label] = precision\n",
        "        recalls[label] = recall\n",
        "        f1_scores[label] = f1\n",
        "\n",
        "    return precisions, recalls, f1_scores\n",
        "\n",
        "# Exemplo de uso\n",
        "if __name__ == \"__main__\":\n",
        "    # Carregar os dados\n",
        "    filepath = \"/content/sample_data/penguins.csv\"\n",
        "    X, y = load_and_preprocess_data(filepath)\n",
        "\n",
        "    # Dividir os dados em treino e teste (exemplo simples)\n",
        "    split_ratio = 0.8\n",
        "    split_index = int(split_ratio * len(X))\n",
        "    X_train, X_test = X[:split_index], X[split_index:]\n",
        "    y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "    # Instanciar o modelo KNN\n",
        "    knn = KNN(k=3)\n",
        "\n",
        "    # Treinar o modelo\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Fazer a previsão\n",
        "    y_pred = knn.predict(X_test)\n",
        "\n",
        "    # Calcular e exibir a matriz de confusão, acurácia, recall, precisão e F1-score\n",
        "    print(\"Matriz de Confusão:\", confusion_matrix(y_test, y_pred))\n",
        "    print(\"Acurácia:\", accuracy(y_test, y_pred))\n",
        "    precisions, recalls, f1_scores = precision_recall_f1(y_test, y_pred)\n",
        "    print(\"Precisões:\", precisions)\n",
        "    print(\"Recalls:\", recalls)\n",
        "    print(\"F1-scores:\", f1_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJcjJ4zfyliS",
        "outputId": "4816516c-b921-4978-9c93-e3d6cf8ce829"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Confusão: {'FEMALE': {'FEMALE': 17, 'MALE': 15}, 'MALE': {'FEMALE': 6, 'MALE': 29}}\n",
            "Acurácia: 0.6865671641791045\n",
            "Precisões: {'FEMALE': 0.7391304347826086, 'MALE': 0.6590909090909091}\n",
            "Recalls: {'FEMALE': 0.53125, 'MALE': 0.8285714285714286}\n",
            "F1-scores: {'FEMALE': 0.6181818181818182, 'MALE': 0.7341772151898734}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste com Naive Bayes"
      ],
      "metadata": {
        "id": "z3cuGPkNy1J1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "class NaiveBayes:\n",
        "    def fit(self, X_train, y_train):\n",
        "        self.classes = np.unique(y_train)\n",
        "        self.mean = {}\n",
        "        self.var = {}\n",
        "        self.priors = {}\n",
        "\n",
        "        for c in self.classes:\n",
        "            X_c = X_train[y_train == c]\n",
        "            self.mean[c] = np.nanmean(X_c, axis=0)\n",
        "            self.var[c] = np.nanvar(X_c, axis=0)\n",
        "            self.priors[c] = X_c.shape[0] / X_train.shape[0]\n",
        "\n",
        "    def gaussian_pdf(self, class_idx, x):\n",
        "        mean = self.mean[class_idx]\n",
        "        var = self.var[class_idx]\n",
        "        numerator = np.exp(-(x - mean) ** 2 / (2 * var))\n",
        "        denominator = np.sqrt(2 * np.pi * var)\n",
        "        return numerator / denominator\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        y_pred = [self._predict(x) for x in X_test]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        posteriors = []\n",
        "\n",
        "        for c in self.classes:\n",
        "            prior = np.log(self.priors[c])\n",
        "            conditional = np.sum(np.log(self.gaussian_pdf(c, x)))\n",
        "            posterior = prior + conditional\n",
        "            posteriors.append(posterior)\n",
        "\n",
        "        return self.classes[np.argmax(posteriors)]\n",
        "\n",
        "def load_and_prepare_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Remover linhas com rótulos nulos\n",
        "    df = df.dropna(subset=[df.columns[-1]])\n",
        "\n",
        "    # Converter as colunas para tipos adequados\n",
        "    df[df.columns[2:4]] = df[df.columns[2:4]].astype(float)\n",
        "    df[df.columns[4:6]] = df[df.columns[4:6]].astype(int)\n",
        "\n",
        "    # Remover linhas com valores numéricos nulos\n",
        "    df = df.dropna(subset=df.columns[2:6])\n",
        "\n",
        "    # Converter rótulo para valores numéricos\n",
        "    y = df[df.columns[-1]].astype('category').cat.codes\n",
        "    X = df.iloc[:, 2:6].values\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def confusion_matrix(y_true, y_pred, classes):\n",
        "    matrix = np.zeros((len(classes), len(classes)), dtype=int)\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        matrix[true][pred] += 1\n",
        "    return matrix\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.sum(y_true == y_pred) / len(y_true)\n",
        "\n",
        "def precision_recall_f1(y_true, y_pred, classes):\n",
        "    metrics = defaultdict(dict)\n",
        "\n",
        "    for c in classes:\n",
        "        tp = np.sum((y_true == c) & (y_pred == c))\n",
        "        fp = np.sum((y_true != c) & (y_pred == c))\n",
        "        fn = np.sum((y_true == c) & (y_pred != c))\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        metrics[c]['precision'] = precision\n",
        "        metrics[c]['recall'] = recall\n",
        "        metrics[c]['f1'] = f1\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Exemplo de uso\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = '/content/sample_data/penguins.csv'\n",
        "\n",
        "    # Carregar e preparar dados\n",
        "    X, y = load_and_prepare_data(file_path)\n",
        "\n",
        "    # Divisão simples para treino e teste (por exemplo, 80% treino, 20% teste)\n",
        "    split_idx = int(0.8 * len(X))\n",
        "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "    # Instanciar o classificador\n",
        "    nb = NaiveBayes()\n",
        "    nb.fit(X_train, y_train)\n",
        "\n",
        "    # Fazer predições\n",
        "    predictions = nb.predict(X_test)\n",
        "\n",
        "    # Calcular a matriz de confusão, acurácia, recall, precisão e f1-score\n",
        "    classes = np.unique(y)\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_test, predictions, classes)\n",
        "    acc = accuracy(y_test, predictions)\n",
        "    metrics = precision_recall_f1(y_test, predictions, classes)\n",
        "\n",
        "    # Exibir resultados\n",
        "    print(\"Matriz de Confusão:\\n\", conf_matrix)\n",
        "    print(\"Acurácia:\", acc)\n",
        "    for c in classes:\n",
        "        print(f\"Classe {c}:\")\n",
        "        print(\" Precisão:\", metrics[c]['precision'])\n",
        "        print(\" Recall:\", metrics[c]['recall'])\n",
        "        print(\" F1-Score:\", metrics[c]['f1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkXlJhAFy4wE",
        "outputId": "264e9bc4-69fe-4076-e41d-994542a10ff0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Confusão:\n",
            " [[ 4 28]\n",
            " [ 0 35]]\n",
            "Acurácia: 0.582089552238806\n",
            "Classe 0:\n",
            " Precisão: 1.0\n",
            " Recall: 0.125\n",
            " F1-Score: 0.2222222222222222\n",
            "Classe 1:\n",
            " Precisão: 0.5555555555555556\n",
            " Recall: 1.0\n",
            " F1-Score: 0.7142857142857143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste com Árvores de Decisão"
      ],
      "metadata": {
        "id": "sMr23-gnzBPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# Função para calcular o índice Gini\n",
        "def gini(groups, classes):\n",
        "    n_instances = float(sum([len(group) for group in groups]))\n",
        "    gini_index = 0.0\n",
        "    for group in groups:\n",
        "        size = len(group)\n",
        "        if size == 0:\n",
        "            continue\n",
        "        score = 0.0\n",
        "        for class_val in classes:\n",
        "            p = [row[-1] for row in group].count(class_val) / size\n",
        "            score += p * p\n",
        "        gini_index += (1.0 - score) * (size / n_instances)\n",
        "    return gini_index\n",
        "\n",
        "# Função para dividir os dados em dois grupos com base em um valor específico de uma coluna\n",
        "def test_split(index, value, dataset):\n",
        "    left, right = [], []\n",
        "    for row in dataset:\n",
        "        if row[index] < value:\n",
        "            left.append(row)\n",
        "        else:\n",
        "            right.append(row)\n",
        "    return left, right\n",
        "\n",
        "# Selecionar a melhor divisão do dataset\n",
        "def get_split(dataset):\n",
        "    class_values = list(set(row[-1] for row in dataset))\n",
        "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "    for index in range(len(dataset[0])-1):\n",
        "        for row in dataset:\n",
        "            groups = test_split(index, row[index], dataset)\n",
        "            gini_index = gini(groups, class_values)\n",
        "            if gini_index < b_score:\n",
        "                b_index, b_value, b_score, b_groups = index, row[index], gini_index, groups\n",
        "    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
        "\n",
        "# Criar um nó folha\n",
        "def to_terminal(group):\n",
        "    outcomes = [row[-1] for row in group]\n",
        "    return max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "# Dividir os nós, criando sub-árvores de forma recursiva ou folhas\n",
        "def split(node, max_depth, min_size, depth):\n",
        "    left, right = node['groups']\n",
        "    del(node['groups'])\n",
        "    if not left or not right:\n",
        "        node['left'] = node['right'] = to_terminal(left + right)\n",
        "        return\n",
        "    if depth >= max_depth:\n",
        "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "        return\n",
        "    if len(left) <= min_size:\n",
        "        node['left'] = to_terminal(left)\n",
        "    else:\n",
        "        node['left'] = get_split(left)\n",
        "        split(node['left'], max_depth, min_size, depth+1)\n",
        "    if len(right) <= min_size:\n",
        "        node['right'] = to_terminal(right)\n",
        "    else:\n",
        "        node['right'] = get_split(right)\n",
        "        split(node['right'], max_depth, min_size, depth+1)\n",
        "\n",
        "# Construir uma árvore de decisão\n",
        "def build_tree(train, max_depth, min_size):\n",
        "    root = get_split(train)\n",
        "    split(root, max_depth, min_size, 1)\n",
        "    return root\n",
        "\n",
        "# Fazer uma previsão com a árvore de decisão\n",
        "def predict(node, row):\n",
        "    if row[node['index']] < node['value']:\n",
        "        if isinstance(node['left'], dict):\n",
        "            return predict(node['left'], row)\n",
        "        else:\n",
        "            return node['left']\n",
        "    else:\n",
        "        if isinstance(node['right'], dict):\n",
        "            return predict(node['right'], row)\n",
        "        else:\n",
        "            return node['right']\n",
        "\n",
        "# Função para calcular a matriz de confusão e as métricas\n",
        "def confusion_matrix_metrics(actual, predicted, classes):\n",
        "    matrix = {label: {label: 0 for label in classes} for label in classes}\n",
        "    for a, p in zip(actual, predicted):\n",
        "        matrix[a][p] += 1\n",
        "\n",
        "    # Calculando métricas\n",
        "    tp = sum(matrix[c][c] for c in classes)  # True Positives\n",
        "    total = len(actual)  # Total de exemplos\n",
        "\n",
        "    # Acurácia\n",
        "    accuracy = tp / total\n",
        "\n",
        "    # Para cada classe, calcular precision, recall e f1-score\n",
        "    precision = {}\n",
        "    recall = {}\n",
        "    f1_score = {}\n",
        "\n",
        "    for c in classes:\n",
        "        tp_c = matrix[c][c]\n",
        "        fp_c = sum(matrix[other][c] for other in classes if other != c)\n",
        "        fn_c = sum(matrix[c][other] for other in classes if other != c)\n",
        "\n",
        "        precision[c] = tp_c / (tp_c + fp_c) if (tp_c + fp_c) > 0 else 0\n",
        "        recall[c] = tp_c / (tp_c + fn_c) if (tp_c + fn_c) > 0 else 0\n",
        "        f1_score[c] = 2 * precision[c] * recall[c] / (precision[c] + recall[c]) if (precision[c] + recall[c]) > 0 else 0\n",
        "\n",
        "    return matrix, accuracy, precision, recall, f1_score\n",
        "\n",
        "# Função para carregar e processar o dataset de pinguins\n",
        "def load_penguin_data(file_path):\n",
        "    dataset = []\n",
        "    with open(file_path, newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Pular cabeçalho\n",
        "        for row in reader:\n",
        "            if not row[6]:  # Ignorar se o rótulo for nulo\n",
        "                continue\n",
        "            processed_row = []\n",
        "            for i in range(7):\n",
        "                if i == 0 or i == 1:  # String (colunas 1 e 2)\n",
        "                    processed_row.append(row[i])\n",
        "                elif i == 2 or i == 3:  # Float (colunas 3 e 4)\n",
        "                    processed_row.append(float(row[i]) if row[i] else 0.0)\n",
        "                elif i == 4 or i == 5:  # Inteiro (colunas 5 e 6)\n",
        "                    processed_row.append(int(row[i]) if row[i] else 0)\n",
        "                else:  # Rótulo (coluna 7)\n",
        "                    processed_row.append(row[i])\n",
        "            dataset.append(processed_row)\n",
        "    return dataset\n",
        "\n",
        "# Exemplo de uso\n",
        "if __name__ == '__main__':\n",
        "    # Carregar o dataset de pinguins\n",
        "    file_path = '/content/sample_data/penguins.csv'\n",
        "    dataset = load_penguin_data(file_path)\n",
        "\n",
        "    # Codificar rótulos como números\n",
        "    class_mapping = {label: i for i, label in enumerate(set(row[-1] for row in dataset))}\n",
        "    for row in dataset:\n",
        "        row[-1] = class_mapping[row[-1]]\n",
        "\n",
        "    # Definindo os parâmetros da árvore\n",
        "    max_depth = 3\n",
        "    min_size = 1\n",
        "\n",
        "    # Construir a árvore\n",
        "    tree = build_tree(dataset, max_depth, min_size)\n",
        "\n",
        "    # Fazer previsões\n",
        "    actual = [row[-1] for row in dataset]\n",
        "    predicted = [predict(tree, row) for row in dataset]\n",
        "\n",
        "    # Calcular matriz de confusão e métricas\n",
        "    classes = list(class_mapping.values())\n",
        "    matrix, accuracy, precision, recall, f1_score = confusion_matrix_metrics(actual, predicted, classes)\n",
        "\n",
        "    print(f\"Matriz de Confusão: {matrix}\")\n",
        "    print(f\"Acurácia: {accuracy}\")\n",
        "    print(f\"Precisão: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1-Score: {f1_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "difu1LNXzF3B",
        "outputId": "1c002457-63be-4e08-dc31-2e61cb2975f1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Confusão: {0: {0: 164, 1: 4}, 1: {0: 24, 1: 141}}\n",
            "Acurácia: 0.9159159159159159\n",
            "Precisão: {0: 0.8723404255319149, 1: 0.9724137931034482}\n",
            "Recall: {0: 0.9761904761904762, 1: 0.8545454545454545}\n",
            "F1-Score: {0: 0.9213483146067415, 1: 0.9096774193548387}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Análise Crítica\n"
      ],
      "metadata": {
        "id": "Pd1csLqbz_VW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. kNN\n",
        "A implementação de KNN gerada pelo ChatGPT segue de forma básica a estrutura do\n",
        "algoritmo discutido em aula, com a definição do número de vizinhos (k), cálculo da dis-\n",
        "tância euclidiana como medida de similaridade, e o voto da maioria dos vizinhos mais\n",
        "próximos. No entanto, algumas diferenças e potenciais melhorias podem ser observadas:\n",
        "\n",
        "\n",
        "*   **Pontos fortes:** A simplicidade da implementação reflete bem o conceito de classi-\n",
        "ficador lazy discutido em aula. O uso de uma função clara para calcular a distância\n",
        "euclidiana e o uso da classe ‘Counter‘ para votação são boas práticas que mantêm\n",
        "o código fácil de ler e entender.\n",
        "*   **Diferenciações e problemas potenciais:** A versão discutida em aula menciona\n",
        "que a escolha de ‘k‘ pode impactar o desempenho do modelo, sendo muito pe-\n",
        "queno propenso a overfitting e muito grande a underfitting. O código não aborda\n",
        "mecanismos para ajuste automático de ‘k‘, algo que poderia ser explorado para\n",
        "evitar esses problemas. Além disso, não foi considerado o uso de pesos para os\n",
        "votos dos vizinhos com base nas distâncias, o que poderia aumentar a robustez a\n",
        "ruídos e tornar o algoritmo menos sensível à escolha de ‘k‘.\n",
        "\n",
        "*   **Opções de projeto e impacto:** O uso da distância euclidiana, embora adequada para muitos casos, pode não ser a escolha ideal dependendo do tipo de dados, como discutido em aula (por exemplo, dados binários assimétricos). Uma sugestão de melhoria seria a inclusão de outras opções de métricas de dissimilaridade que se adequem a diferentes tipos de dados, como a distância de Manhattan.\n",
        "*   **Sugestões de melhorias:** Além de ponderar os votos por distância, a inclusão de uma estrutura de dados eficiente, como KD-Trees, poderia melhorar a eficiência computacional, especialmente para grandes conjuntos de dados, como discutido na aula sobre o impacto do alto custo computacional no KNN.\n",
        "\n",
        "Isso demonstra como o algoritmo gerado pelo GPT pode ser um ponto de partida\n",
        "válido, mas melhorias e ajustes são necessários para alinhar melhor o código aos pontos\n",
        "discutidos em sala."
      ],
      "metadata": {
        "id": "_b7zklRD1lHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Naive Bayes\n",
        "A implementação do Naive Bayes gerada pelo GPT tambem segue bem a estrutura do\n",
        "classificador discutido em aula, usando o Teorema de Bayes para calcular a probabilidade\n",
        "posterior e assumindo independência entre os atributos. No código, a suposição de que os\n",
        "atributos seguem uma distribuição Gaussiana para cada classe foi corretamente aplicada,\n",
        "o que é uma escolha comum para atributos contínuos.\n",
        "\n",
        "\n",
        "*   **Pontos fortes:** O algoritmo é eficiente, realizando o cálculo das probabilidades\n",
        "por meio de uma única varredura nos dados de treino e armazenando os parâme-\n",
        "tros necessários (média, variância e prior) para cada classe, o que reflete o que foi\n",
        "discutido em aula sobre a eficiência do Naive Bayes. Além disso, o uso de loga-\n",
        "ritmos para evitar underflow numérico ao multiplicar pequenas probabilidades é\n",
        "uma escolha inteligente, garantindo maior estabilidade numérica.\n",
        "*   **Diferenças e potenciais problemas:** Uma diferença em relação ao que foi visto\n",
        "em aula é que a implementação assume uma distribuição Gaussiana para os atribu-\n",
        "tos, o que pode não ser ideal se os dados forem categóricos ou não seguirem essa\n",
        "distribuição. A aula também menciona a robustez do Naive Bayes para lidar com\n",
        "atributos irrelevantes, o que não está explícito no código. No entanto, o modelo\n",
        "em si herda essa característica, mesmo que o código não faça ajustes específicos\n",
        "para detectar atributos irrelevantes.\n",
        "*   **Opções de projeto e impacto:** O uso da distribuição Gaussiana como densidade\n",
        "de probabilidade é uma escolha projetual que funciona bem para dados contínuos,\n",
        "mas a implementação poderia ser expandida para incluir outros tipos de distribui-\n",
        "ção (como a Bernoulli para atributos binários). Isso ampliaria a aplicabilidade do\n",
        "código em diferentes cenários de dados.\n",
        "*   **Sugestões de melhorias:** Seria interessante adicionar um tratamento mais flexível\n",
        "para lidar com atributos de diferentes naturezas (discretos e contínuos), algo que\n",
        "o Naive Bayes faz bem em teoria, mas não está explícito nesta implementação.\n",
        "Além disso, a inclusão de um processo de seleção de atributos poderia ser útil\n",
        "para lidar com redundâncias, conforme discutido em aula.\n",
        "\n",
        "Vemos que o código gerado pelo GPT está alinhado com os princípios fundamen-\n",
        "tais do Naive Bayes, mas poderia ser refinado para abranger mais cenários e aumentar sua\n",
        "flexibilidade."
      ],
      "metadata": {
        "id": "eHpJDw2K2HVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Árvores de Decisão\n",
        "Comparando o algoritmo fornecido com o que foi ensinado em sala de aula, podemos\n",
        "identificar vários pontos de convergência e algumas áreas onde há espaço para discussão.\n",
        "Vamos analisá-los em termos de conceitos e decisões de design:\n",
        "**Semelhanças**\n",
        "\n",
        "\n",
        "*   **Estrutura básica:** O algoritmo implementado segue a ideia central do método de\n",
        "indução top-down das árvores de decisão, que é o algoritmo de Hunt. Ele parte\n",
        "de um conjunto de dados e divide os registros com base no atributo que otimiza\n",
        "localmente determinado critério de divisão (neste caso, o índice Gini), tal como\n",
        "discutido em aula.\n",
        "*   **Uso do índice Gini:** O código utiliza o índice Gini para avaliar a pureza dos nós,\n",
        "o que está de acordo com as métricas de impureza discutidas. Em aula, vimos três\n",
        "principais medidas de impureza (índice Gini, entropia e erro de classificação), e o\n",
        "Gini é bastante comum na prática devido à sua simplicidade.\n",
        "*   **Recursividade:** A árvore é construída de maneira recursiva, e as divisões conti-\n",
        "nuam até se alcançar critérios de parada, como profundidade máxima da árvore\n",
        "ou o tamanho mínimo de instâncias em um nó, alinhado com a estratégia gulosa\n",
        "vista em sala.\n",
        "*   **Divisões binárias:** O algoritmo implementa apenas divisões binárias para atribu-\n",
        "tos contínuos, uma escolha comum para simplificar o processo e que também foi\n",
        "abordada nas aulas.\n",
        "\n",
        "\n",
        "\n",
        "**Diferenças e pontos de discussão**\n",
        "\n",
        "\n",
        "*   **Critério de parada:** No algoritmo fornecido, os critérios de parada incluem o li-\n",
        "mite de profundidade da árvore (max_depth) e o número mínimo de instâncias em\n",
        "um nó (min_size). Em aula, foi discutido que outros critérios de parada poderiam\n",
        "ser usados, como a homogeneidade das classes ou a impossibilidade de melhorar o\n",
        "critério de divisão, por exemplo, quando todos os atributos restantes têm o mesmo\n",
        "valor. O código não leva em consideração diretamente a pureza do nó ao decidir\n",
        "parar, o que poderia ser uma adição interessante.\n",
        "*   **Divisão para atributos categóricos:** O código fornecido parece focado em atri-\n",
        "butos contínuos (pelo menos no exemplo dado). Em sala de aula, discutiu-se a\n",
        "divisão tanto para atributos contínuos quanto categóricos (nominais e ordinais). O\n",
        "código atual teria que ser adaptado para lidar com atributos categóricos, especial-\n",
        "mente para suportar divisões múltiplas, que também foram abordadas em aula.\n",
        "*   **Critério de escolha do atributo:** O algoritmo sempre escolhe o atributo com a\n",
        "menor impureza medida pelo índice Gini, o que está correto. No entanto, poderia\n",
        "ser interessante comparar o desempenho com outros critérios discutidos, como a\n",
        "**entropia** ou o **erro de classificação**, para observar o impacto no modelo\n",
        "gerado. Isso foi mencionado como uma questão de projeto importante em sala.\n",
        "*   **Complexidade computacional:** O código implementa uma abordagem de força\n",
        "bruta, onde cada valor de cada atributo é testado como um possível ponto de di-\n",
        "visão, recalculando o índice Gini para cada divisão possível. Em aula, foi menci-\n",
        "onado que essa abordagem pode ser computacionalmente pesada, especialmente\n",
        "para grandes conjuntos de dados. Uma possível otimização discutida seria o pré-\n",
        "processamento dos atributos contínuos para calcular os limiares de divisão de\n",
        "forma mais eficiente.\n",
        "*   **Árvores não otimizadas:** Conforme discutido em sala, encontrar a árvore ótima\n",
        "é um problema NP-difícil, e heurísticas são utilizadas para se obter uma solu-\n",
        "ção satisfatória. O algoritmo fornecido utiliza uma abordagem gulosa (greedy),\n",
        "tomando decisões localmente ótimas (minimização do índice Gini em cada nó),\n",
        "mas não garante a árvore globalmente ótima, o que foi mencionado como uma\n",
        "limitação geral das árvores de decisão.\n",
        "\n",
        "**Outras Considerações**\n",
        "\n",
        "\n",
        "\n",
        "*   **Visualização da árvore:** Em sala de aula, a facilidade de interpretação das árvores\n",
        "de decisão foi destacada, porém o algoritmo não inclui uma funcionalidade para\n",
        "visualizar a árvore gerada, o que seria útil para verificar a compreensibilidade das\n",
        "regras criadas.\n",
        "*   **Ajuste de overfitting:** Um aspecto importante das árvores de decisão é o risco\n",
        "de overfitting em dados de treino. O algoritmo atual utiliza profundidade máxima\n",
        "e tamanho mínimo dos nós como forma de controle, mas em sala de aula, foi\n",
        "mencionado que técnicas como **poda** (pruning) também podem ser úteis para\n",
        "controlar o ajuste excessivo, algo que não foi abordado no código.\n",
        "\n",
        "O código fornecido está alinhado com muitos dos conceitos e técnicas vistos em\n",
        "aula, especialmente no que diz respeito ao uso do índice Gini, à recursividade e à divisão\n",
        "binária de atributos contínuos. No entanto, poderia ser enriquecido ao incorporar suporte\n",
        "a atributos categóricos, critérios adicionais para escolha de atributos e mais técnicas de\n",
        "controle de overfitting, como a poda."
      ],
      "metadata": {
        "id": "x645wAyo2flt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Conclusão"
      ],
      "metadata": {
        "id": "tgIgYVn31xMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A comparação entre os três algoritmos vistos em aula — Árvore de Decisão, K-Nearest\n",
        "Neighbors (KNN) e Naive Bayes — e o algoritmo desenvolvido a partir do modelo GPT-\n",
        "4 evidenciou as semelhanças e distinções fundamentais entre os métodos tradicionais e\n",
        "a abordagem automatizada. Enquanto a Árvore de Decisão oferece uma estrutura inter-\n",
        "pretável e eficiente para determinados conjuntos de dados, o KNN se destaca pela sim-\n",
        "plicidade e robustez, embora sofra com grandes volumes de dados. O Naive Bayes, por\n",
        "sua vez, provou ser altamente eficiente em cenários com forte independência entre as\n",
        "variáveis, mas pode ser limitado em outros contextos. O algoritmo gerado pelo GPT-4,\n",
        "embora tenha demonstrado um bom desempenho em termos de precisão e flexibilidade,\n",
        "destacou-se pela facilidade com que foi adaptado e ajustado sem o uso de bibliotecas pré-\n",
        "definidas. A análise crítica dessas implementações reforçou a importância de entender os\n",
        "fundamentos teóricos para ajustar e otimizar algoritmos conforme as necessidades espe-\n",
        "cíficas de cada problema, consolidando os conceitos apresentados em sala de aula com as\n",
        "práticas aplicadas no desenvolvimento de modelos do zero."
      ],
      "metadata": {
        "id": "Q6tYZMUx13nD"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}